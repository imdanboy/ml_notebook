{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "* author : Dan Lim\n",
    "* language : python\n",
    "* required library : numpy\n",
    "* purpose : simple example for neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60290, 13)\n",
      "(60290,)\n",
      "(18490, 13)\n",
      "(18490,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train = np.loadtxt('trn.txt')\n",
    "test = np.loadtxt('tst.txt')\n",
    "\n",
    "X_train = train[:, :-1]\n",
    "y_train = train[:, -1]\n",
    "X_test = test[:, :-1]\n",
    "y_test = test[:, -1]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiLayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    def __init__(self, hidden_layer_sizes=(100,), learning_rate=0.01, momentum=0.9, batch_size=200,\n",
    "                 n_epoch=100, seed=24, verbose=False):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.n_hidden_layer = len(hidden_layer_sizes)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epoch = n_epoch\n",
    "        self.seed = seed\n",
    "        self.verbose = verbose\n",
    "    def __initialize(self, X):\n",
    "        np.random.seed(self.seed)\n",
    "        self.W = [None for _ in range(self.n_hidden_layer+1)] # weigth\n",
    "        self.b = [None for _ in range(self.n_hidden_layer+1)] # bias\n",
    "        self.DT = [None for _ in range(self.n_hidden_layer+1)] # delta\n",
    "        self.Z = [None for _ in range(self.n_hidden_layer+2)] # weighted sum\n",
    "        self.O = [None for _ in range(self.n_hidden_layer+2)] # neuron output (activated neuron)\n",
    "        self.WM = [0 for _ in range(len(self.W))] # weight momentum\n",
    "        self.bM = [0 for _ in range(len(self.b))] # bias momentum\n",
    "        \n",
    "        self.W[0] = np.random.normal(0,1, (self.hidden_layer_sizes[0], X.shape[1]))\n",
    "        for i in range(self.n_hidden_layer-1):\n",
    "            self.W[i+1] = np.random.normal(0,1, (self.hidden_layer_sizes[i+1], self.hidden_layer_sizes[i]))\n",
    "        self.W[-1] = np.random.normal(0,1, (1, self.hidden_layer_sizes[-1]))\n",
    "        \n",
    "        for i in range(self.n_hidden_layer):\n",
    "            self.b[i] = np.random.normal(0,1, (self.hidden_layer_sizes[i],1))\n",
    "        self.b[-1] = np.random.normal(0,1, (1,1))\n",
    "    def __activate(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    def __forward_propagate(self, X):\n",
    "        self.Z[0] = X.transpose().copy()\n",
    "        self.O[0] = self.Z[0].copy()\n",
    "        for i in range(self.n_hidden_layer+1):\n",
    "            self.Z[i+1] = np.dot(self.W[i], self.O[i]) + self.b[i]\n",
    "            self.O[i+1] = self.__activate(self.Z[i+1])\n",
    "        return self.O[-1]\n",
    "    def __weight_update(self, X, y):\n",
    "        outputs = self.__forward_propagate(X)\n",
    "        y = y.reshape(1,-1)\n",
    "        \n",
    "        self.DT[-1] = outputs - y\n",
    "        for i in reversed(range(self.n_hidden_layer)):\n",
    "            self.DT[i] = self.O[i+1] * (1-self.O[i+1]) * np.dot(self.W[i+1].transpose(), self.DT[i+1])\n",
    "        \n",
    "        for i in range(self.n_hidden_layer+1):\n",
    "            W_prev = self.W[i].copy()\n",
    "            b_prev = self.b[i].copy()\n",
    "            self.W[i] = self.W[i] - self.learning_rate * (np.dot(self.DT[i], self.O[i].transpose())\n",
    "                                                          / X.shape[0]) + self.momentum*self.WM[i]\n",
    "            self.b[i] = self.b[i] - self.learning_rate * (np.dot(self.DT[i], np.ones((X.shape[0],1)))\n",
    "                                                          / X.shape[0]) + self.momentum*self.bM[i]\n",
    "            self.WM[i] = self.W[i] - W_prev\n",
    "            self.bM[i] = self.b[i] - b_prev\n",
    "        \n",
    "    def __log_loss(self, X, y):\n",
    "        outputs = self.__forward_propagate(X)\n",
    "        outputs = outputs.reshape(-1)\n",
    "        outputs = np.clip(outputs, 1e-10, 1-1e-10)\n",
    "        return -np.mean(y*np.log(outputs) + (1-y)*np.log(1-outputs))\n",
    "    def fit(self, X, y):\n",
    "        self.__initialize(X)\n",
    "        for epoch in range(self.n_epoch):\n",
    "            for i in range(0, X.shape[0], self.batch_size):\n",
    "                self.__weight_update(X[i:(i+self.batch_size),:], y[i:(i+self.batch_size)])\n",
    "            if self.verbose:\n",
    "                print('epoch[%d/%d], loss = %f' % (epoch+1, self.n_epoch, self.__log_loss(X, y)))\n",
    "    def predict(self, X):\n",
    "        outputs = self.__forward_propagate(X)\n",
    "        outputs = outputs.reshape(-1)\n",
    "        outputs[outputs>=0.5] = 1\n",
    "        outputs[outputs<0.5] = 0\n",
    "        return outputs\n",
    "    def score(self, X, y):\n",
    "        predicted = self.predict(X)\n",
    "        return np.mean(y == predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# selected hyper parameters\n",
    "\n",
    "* initial weights : randomly drawn from gaussian distribution ~ N(0,1)\n",
    "* number of layers : 2 hidden layer\n",
    "* number of nodes in each layers : (13, 40, 20, 1) := (input, hidden1, hidden2, output)\n",
    "* learning rate : 0.1\n",
    "* momentum : 0.9\n",
    "* number of epochs : 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlp = MLP(hidden_layer_sizes=(60,30), learning_rate=0.1, momentum=0.9, batch_size=200, n_epoch=100, verbose=False)\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# error rate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.39%\n"
     ]
    }
   ],
   "source": [
    "score = mlp.score(X_test, y_test)\n",
    "print('%.2f%%' % ((1 - score) * 100))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
